# Resources  
### Introductory Resources  
[Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)  
> This article describes the theory of a neural network quickly and simply. It should be used as one of the first introductory resources. It also includes references to more helpful resources.

[How Deep Neural Networks Work](https://brohrer.github.io/how_neural_networks_work.html)  
> This video describes the functions of a very simple neural network. It focuses on a mathematical walkthrough with a pattern of four black and white pixels. It gives the inputs, weights, biases, and functions values. It should be used in conjunction with other introductory resources; not the first introduction.

Heaton, Jeff. [*Introduction to Neural Networks with Java.*](https://www.amazon.com/Introduction-Neural-Networks-Java-2nd/dp/1604390085) 2nd ed., Heaton Research, 1 Oct. 2008.  
> This book is an excellent first introduction to feedforward neural networks, with additions for the other types of networks. It works even better for students coming right out of APCS. I would recommend the first five chapters (bolded below) to learn introductory neural networks. Beyond the first five chapters, the book enters more advanced learning algorithms.
- **Overview of Neural Networks**
- **Matrix Operations**
- **Using a Hopfield Neural Network**
- **How a Machine Learns**
- **Feedforward Neural Networks**
- Understanding Genetic Algorithms
- Understanding Simulated Annealing
- Pruning Neural Networks
- Predictive Neural Networks
- Application to the Financial Markets
- Understanding the Self-Organizing Map
- OCR with the Self-Organizing Map
- Bot Programming and Neural Networks
- The Future of Neural Networks
- Downloading Examples
- Mathematical Background
- Common Threshold Functions
- Executing Examples

### First Project Resources
Nielsen, Michael. [*Neural Networks and Deep Learning.*](http://neuralnetworksanddeeplearning.com/) Determination Press, 2015.
> This online book requires a small precursor to machine learning and a solid grasp of calculus, but is immeasurably helpful for learning the core of feedforward/backprop neural networks. If the programmer doesn't know calculus, the book can just be used to get the code that deals with the math (treat it like a black box).  
- Using Neural Nets to Recognize Handwritten Digits
- How the Backpropagation Algorithm Works
- Improving the Way Neural Networks Learn
- A Visual Proof that Neural Nets can Compute any Function
- Why are Deep Neural Networks Hard to Train?
- Deep Learning

[Neural Networks: A Visual Introduction](https://youtu.be/bVQUSndDllU?list=PLFt_AvWsXl0frsCrmv4fKfZ2OQIwoUuYO)
> This playlist of videos introduces the idea of a neural network in the context of plotting input data points and graphs and having the algorithm sort the inputs using an inequality equation. It also includes a tutorial for programming them using Python. It should be used after the student has been introduced to neural networks, but before the difficult mathematical theory.

Fausett, Laurene V. [*Fundamentals of Neural Networks: Architectures, Algorithms And Applications.*](https://www.amazon.com/Fundamentals-Neural-Networks-Architectures-Applications/dp/0133341860) 1st ed., Pearson, 19 Dec. 1993.
> This book contains examples of neural networks and their various implementations and applications. The first two chapters would probably be most useful for the completion of an initial project.  
- **Introduction**
- **Simple Neural Nets for Pattern Classification**
- Pattern Association
- Neural Networks Based on Competition
- Adaptive Resonance Theory
- Backpropagation Neural Net
- A Sampler of Other Neural Nets



### For Further Exploration
Downing, Keith L. [*Intelligence Emerging: Adaptivity and Search in Evolving Neural Systems.*](https://mitpress.mit.edu/books/intelligence-emerging) MIT Press, May 2015.
> This book is a full description of neural networks, different methods of training, and their relationships with neuroscience. It focuses much more heavily on the principles of intelligence, both biologically and artificially, rather than the actual construction of neural networks.
- Introduction
- Emergence
- Search: The Core of AI
- Representations for Search and Emergence
- Evolutionary Algorithms
- Artificial Neural Networks
- Knowledge Representation in Neural Networks
- Search and Representation in Evolutionary Algorithms
- Evolution and Development of the Brain
- Learning via Synaptic Tuning
- Trial-and-Error Learning in Neural Networks
- Evolving Artificial Neural Networks
- Recognizing Emergent Intelligence
- Conclusion
- Evolutionary Algorithm Sidelights
- Neural Network Sidelights

[Example of Neural Network Backpropagation Imagination](https://youtu.be/uixGgMInc48?t=168)
> This is a demonstration of a program that uses Gauss Sampling to have a feedforward/backprop neural network 'imagine' potential outputs. This is one potential follow-up on the Python MNIST project. The more common approach to this problem is a Generative Adversarial Network. The article directly below provides an introduction to GANs.

[A Beginner's Guide to Generative Adversarial Networks (GANs)](https://skymind.ai/wiki/generative-adversarial-network-gan)
> This article provides an introduction to Generative Adversarial Networks, which are designed to imagine potential inputs to the network. This might be an easier approach to that problem than Gauss Sampling.

Schmidhuber, Jürgen. [“Deep Learning in Neural Networks: An Overview.”](arxiv.org/pdf/1404.7828v4.pdf) *Neural Networks*, edited by Kenji Doya and DeLiang Wang, vol. 61, Jan. 2015, pp. 85-117. arxiv.org/pdf/1404.7828v4.pdf.
> The first four sections of this article (bolded) are an excellent overview of the more advanced concepts in neural networks once a student understands feedforward ANNs. The fifth section provides a history of neural networks which is mostly useless to an introductory computer science student.
- Introduction to Deep Learning (DL) in Neural Networks (NNs)
- Event-Oriented Notation for Activation Spreading in NNs
- Depth of Credit Assignment Paths (CAPs) and of Problems
- Recurring Themes of Deep Learning
- Supervised NNs, Some Helped by Unsupervised NNs
- DL in FNNs and RNNs for Reinforcement Learning (RL)
- Conclusion and Outlook

Ripley, Brian D. [“Neural Networks and Related Methods for Classification.”] *Journal of the Royal Statistical Society Series B*, edited by D. Dunson and S. Wood, vol. 56, no. 3, 1994, pp. 409-456.
> This article introduces symmetric recurrent neural networks, and compares them to feedforward neural networks. Both types use supervised learning.

### For Math Enthusiasts
Krose, Ben, and Patrick Smagt. [“An Introduction to Neural Networks.”](https://www.infor.uva.es/~teodoro/neuro-intro.pdf) 8th ed., U of Amsterdam, Nov. 1996.
> This article is not necessary for any high school student to understand neural networks, but it can be helpful if the student likes the math a lot. Much of it is far too complex and mathematical for any high school student, but given time and effort somebody would be able to understand it.
- Fundamentals
- Theory
- Applications
- Implementations

Glorot, Xavier, and Yoshua Bengio. [“Understanding the Difficulty of Training Deep Feedforward Neural Networks.”](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi) U of Montréal, 2010
> This article describes training deep feedforward neural networks using data. It studies gradients, backpropagation, and introduces a lot of complicated math to summarize the topic.

### Reinforcement Learning
[A Beginner’s Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning)  
> This article describes the basics of a reinforcement learning neural network, drawing from the first edition of Reinforcement Learning: An Introduction.

Sutton, Richard S., and Andrew G. Barto. [*Reinforcement Learning: An Introduction.*](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=dp_ob_title_bk) Adaptive Computation and Machine Learning, edited by Francis Bach, 2nd ed., Bradford, 13 Nov. 2018.
> This book provides a complete overview of reinforcement learning, which is used when the correct decision in a certain situation isn't known, but the program can measure its performance with some sort of reward. The first three chapters (bolded) are essential for any reinforcement learning project, and can be supported by the second resource (directly below). Future chapters delve into more specific approaches to different reinforcement learning problems.
- **Introduction**
- **Multi-armed Bandits**
- **Finite Markov Decision Processes**
- Dynamic Programming
- Monte Carlo Methods
- Temporal-Difference Learning
- n-step Bootstrapping
- Planning and Learning with Tabular Methods
- On-policy Prediction with Approximation
- On-policy Control with Approximation
- *Off-policy Methods with Approximation
- Eligibility Traces
- Policy Gradient Methods
- Psychology
- Neuroscience
- Applications and Case Studies
- Frontiers

[Bandit Problems](https://oneraynyday.github.io/ml/2018/05/03/Reinforcement-Learning-Bandit/), [Markov Decision Processes](https://oneraynyday.github.io/ml/2018/05/06/Reinforcement-Learning-MDPs/), [Monte Carlo](https://oneraynyday.github.io/ml/2018/05/24/Reinforcement-Learning-Monte-Carlo/)  
> This collection of articles supports the first three chapters from *Reinforcement Learning: An Introduction*. It isn't very helpful on its own, but provides code examples of the concepts discussed in the book. (Note: The author of the articles is sporadically releasing new ones, so there might be articles supporting more chapters in the future.)  
